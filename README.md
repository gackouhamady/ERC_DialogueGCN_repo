# Re-evaluating DialogueGCN for Emotion Recognition in Conversation


[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

## ğŸ“˜ Project Overview

This project conducts a **critical reproduction and evaluation** of the DialogueGCN model â€” a graph-based neural network designed for **Emotion Recognition in Conversations (ERC)**. Our work involves faithful reimplementation, empirical validation on benchmark datasets, and exploration of its practical and architectural limitations.

## ğŸ¯ Objectives

- Reproduce the original DialogueGCN architecture and training setup.
- Evaluate its performance on **IEMOCAP**, **MELD**, and **DailyDialog** datasets.
- Analyze the effects of key hyperparameters (dropout, learning rate, context window).
- Test the model's **scalability** and **hardware sensitivity**, especially on **EmoryNLP** (CPU vs GPU).
- Provide clear insights into **graph-based architectures** for conversational emotion analysis.

## ğŸ§  Why DialogueGCN?

DialogueGCN (Ghosal et al., EMNLP-IJCNLP 2019) models dialogues as relational graphs to capture both **temporal** and **inter-speaker dependencies**, using Relational GCN layers. Its design allows for fine-grained modeling of interpersonal emotional dynamics, surpassing sequential baselines (LSTM, GRU, DialogueRNN) in accuracy and expressivity.

## ğŸ”¬ Key Results

```bash

| Dataset      | F1-score (original) | F1-score (reproduced) | Notes                                           |
|--------------|---------------------|------------------------|------------------------------------------------|
| IEMOCAP      | 64.18%              | 63.9%                  | Stable with weighted loss + dropout            |
| DailyDialog  | N/A                 | 82.28%                 | Excellent generalization, clean structure      |
| MELD         | 58.10%              | 48.12%                 | Convergence issues, unbalanced label problem   |
| EmoryNLP     | --                  | --                     | Failed training on CPU (memory bottleneck)     |
```
## âš™ï¸ Technologies

- Python 3.10
- PyTorch 2.7.0
- PyTorch Geometric 2.6.1
- GloVe embeddings (300d)
- TensorBoard, Pandas, scikit-learn

## ğŸ“‚ Project Structure









## ğŸ“ Structure du projet
```bash
ERC_DIALOGUEGCN_HAMADY/
â”œâ”€â”€ .vscode/                      # Configurations VSCode
â”œâ”€â”€ data/                         # Datasets et fichiers prÃ©traitÃ©s
â”œâ”€â”€ glove/                        # Fichiers d'embedding GloVe
â”œâ”€â”€ models/                       # ImplÃ©mentations de modÃ¨les (DialogueGCN, etc.)
â”œâ”€â”€ report/                       # Rapport scientifique (PDF, .tex, etc.)
â”œâ”€â”€ scripts/                      # Scripts d'entraÃ®nement par dataset
â”‚   â”œâ”€â”€ saved/                    # Checkpoints sauvegardÃ©s
â”‚   â”œâ”€â”€ train_evaluate_dailydialogue.py
â”‚   â”œâ”€â”€ train_evaluate_emory.py
â”‚   â”œâ”€â”€ train_evaluate_iemocap.py
â”‚   â””â”€â”€ train_evaluate_meld.py
â”œâ”€â”€ utils/                        # Utilitaires Python
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ daily_dialog_loader/     # Loader spÃ©cifique pour DailyDialog
â”‚   â””â”€â”€ video_presentation/      # Ã‰lÃ©ments pour la vidÃ©o de prÃ©sentation
â”œâ”€â”€ .gitignore                    # Fichiers ignorÃ©s par Git
â”œâ”€â”€ discussion.ouverte.txt       # Notes ouvertes de discussion ou TODO
â”œâ”€â”€ LICENSE                       # Licence du projet
â”œâ”€â”€ preprocess_dailydialog.py    # Script de prÃ©traitement pour DailyDialog
â”œâ”€â”€ preprocess_emorynlp.py       # Script de prÃ©traitement pour EmoryNLP
â”œâ”€â”€ preprocess_meld.py           # Script de prÃ©traitement pour MELD
â”œâ”€â”€ README.md                    # PrÃ©sentation du projet
â”œâ”€â”€ requirements.txt             # DÃ©pendances Python
â”œâ”€â”€ results_resume.txt           # RÃ©sumÃ© synthÃ©tique des rÃ©sultats
â””â”€â”€ results.txt                  # RÃ©sultats dÃ©taillÃ©s


```

## ğŸ§ª Reproducibility

All hyperparameters, Scripts, and model checkpoints are provided. To replicate the main results

##ğŸ” Highlights
- Graph-based architectures significantly outperform sequential models in structured ERC.

- Training fails on CPU for EmoryNLP due to O(NÂ²) memory cost of relational GCN.

- Small batch sizes and dot-product attention yield the best F1 scores.

- Class weighting crucial for imbalanced datasets like MELD.

ğŸ“ˆ Future Directions
- Integrate multimodal features (audio, video) for richer emotion cues.

- Explore graph sparsification for efficient CPU training.

- Apply explainability techniques (e.g., GNNExplainer).

- Investigate transfer learning to social media or low-resource domains.

## ğŸ“š References ClÃ©s
- Ghosal et al., DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation, EMNLP-IJCNLP 2019.

- Majumder et al., DialogueRNN, AAAI 2019.

- Poria et al., MELD Dataset, ACL 2019.

- Schlichtkrull et al., Relational GCN, ESWC 2018.

#### Author: Hamady GACKOU
#### Supervisor: Dr. SÃ©verine AFFELT, UniversitÃ© Paris CitÃ©
## ğŸ“… Project Year: 2025
## ğŸ“„ See full report in /report/







## ğŸ“„ Licence
[See](LICENSE)

